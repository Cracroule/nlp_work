{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model, save_model, Model\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "DATA_PATH = \"./data/\"\n",
    "\n",
    "DISPLAY_GRAPH = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_list(text):\n",
    "    \"\"\"\" split input text to a list of words, considering most punctuation as words \"\"\"\n",
    "    # textl = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "    # textl = re.findall(r\"[a-z']+|[.,:\\'!?;()\\\"-]\", text)  # punctuation management, might be improved (\\n ?)\n",
    "    return re.findall(r\"[\\w]+|[.,':!?;()\\\"-]\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file, additional_dimensions=0, max_vocabulary_size=None, keep_only_filter=None):\n",
    "    \"\"\"\n",
    "    Load glove matrix (word representation).\n",
    "\n",
    "    Arguments:\n",
    "    glove_file -- str, file to import glove word representation (matrix of weights)\n",
    "    additional_dimensions -- (optional) int, Nb of additional dimensions to add to loaded vectors (filled with zeros)\n",
    "    max_vocabulary_size -- (optional) int, nb max of loaded words. Use it to reduce dimensionality\n",
    "    keep_only_filter -- (optional) iterable, seq of words aimed to be loaded in glove file (ignore words not in filter).\n",
    "\n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "\n",
    "    \"\"\"\n",
    "    with open(glove_file, 'r', encoding='utf8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            if max_vocabulary_size and i >= max_vocabulary_size:\n",
    "                # we have enough words, we stop adding them\n",
    "                break\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            if keep_only_filter and curr_word not in keep_only_filter:\n",
    "                # if word does not appear in filter (in input text?) we don t keep it\n",
    "                continue\n",
    "            words.add(curr_word)\n",
    "            if not additional_dimensions:\n",
    "                word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            else:\n",
    "                word_to_vec_map[curr_word] = np.array(line[1:] + ['0.'] * additional_dimensions, dtype=np.float64)\n",
    "            i += 1\n",
    "\n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_and_glove(text_file, glove_file, replace_by_token=True, max_vocabulary_size=None, display_analysis=True):\n",
    "    \"\"\" Load text and glove matrix from files. Split input text as a list, considering most punctuation as words.\n",
    "      Replace unknown words and nouns by corresponding tokens.\n",
    "      Add those tokens in vocabulary and within glove representation by adding 2 dimensions.\n",
    "      return textl, word_to_index, index_to_word, word_to_vec_map\"\"\"\n",
    "\n",
    "    # text = io.open(text_file, encoding='utf-8').read().lower()\n",
    "    texT = io.open(text_file, encoding='utf-8').read()\n",
    "    text = texT.lower()\n",
    "\n",
    "    textl = text_to_list(text)  # without capital letters\n",
    "    texTl = text_to_list(texT)  # with capital letters (we assume: unknown word has a capital letter then it's a noun)\n",
    "\n",
    "    textl_set = set(textl)   # list of unique words, we'll filter gloave matrix on them\n",
    "    # load initial glove representation, we add 2 dimensions for noons and ukn tokens\n",
    "    word_to_index, index_to_word, word_to_vec_map = read_glove_vecs(glove_file, additional_dimensions=2,\n",
    "                                                                    max_vocabulary_size=max_vocabulary_size,\n",
    "                                                                    keep_only_filter=textl_set)\n",
    "    representation_dimensionality = word_to_vec_map[index_to_word[1]].shape[0]\n",
    "\n",
    "    nb_of_words = len(textl)\n",
    "    vocab_size = len(word_to_index)\n",
    "    nouns, unknown_words = list(), list()\n",
    "\n",
    "    # add <NOUN> and <UKN> tokens to vocabulary and glove representation\n",
    "    nouns_token, unknown_token = \"<NOUN>\", \"<UKN>\"\n",
    "    index_to_word[vocab_size + 1] = nouns_token\n",
    "    index_to_word[vocab_size + 2] = unknown_token\n",
    "    word_to_index[nouns_token] = vocab_size + 1\n",
    "    word_to_index[unknown_token] = vocab_size + 2\n",
    "    noun_vect = [0.] * representation_dimensionality\n",
    "    noun_vect[-2] = 1\n",
    "    ukn_vect = [0.] * representation_dimensionality\n",
    "    ukn_vect[-1] = 1\n",
    "    word_to_vec_map[nouns_token] = np.array(noun_vect, dtype=np.float64)\n",
    "    word_to_vec_map[unknown_token] = np.array(ukn_vect, dtype=np.float64)\n",
    "\n",
    "    working_words = 0\n",
    "    for i, w in enumerate(textl):\n",
    "        try:\n",
    "            _ = word_to_index[w]\n",
    "            working_words += 1\n",
    "\n",
    "        except KeyError:\n",
    "            if w not in unknown_words and w not in nouns:\n",
    "                # print(\"word number\", i, \"does not exist:\", w)\n",
    "                if textl[i] == texTl[i]:  # i.e. if there is a capital letter difference\n",
    "                    unknown_words.append(w)\n",
    "                else:\n",
    "                    nouns.append(w)\n",
    "\n",
    "            if replace_by_token:\n",
    "                if w in unknown_words:\n",
    "                    textl[i] = unknown_token\n",
    "                else:\n",
    "                    textl[i] = nouns_token\n",
    "\n",
    "    if display_analysis:\n",
    "        print(\"nb of words:\", nb_of_words)\n",
    "        # print(unknown_token, len(unknown_words), unknown_words)\n",
    "        # print(nouns_token, len(nouns), nouns)\n",
    "        print(\"% of known words in text\", working_words/nb_of_words)\n",
    "\n",
    "    return textl, word_to_index, index_to_word, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices_simple(txt, word_to_index):\n",
    "    \"\"\" converts input text to indices \"\"\"\n",
    "    txtl = text_to_list(txt.lower())\n",
    "    txTl = text_to_list(txt)\n",
    "    txt_indices = np.zeros((1, len(txtl)), dtype=np.int32)\n",
    "    for i, w in enumerate(txtl):\n",
    "        try:\n",
    "            w_i = word_to_index[w]\n",
    "        except KeyError:\n",
    "            if w != txTl[i]:  # capital letter difference -> it's a Noun !\n",
    "                w_i = word_to_index['<NOUN>']\n",
    "            else:  # not a capital letter difference -> unknown word !\n",
    "                w_i = word_to_index['<UKN>']\n",
    "        txt_indices[0, i] = w_i\n",
    "    return txt_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index, index_to_word):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "\n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_len = len(word_to_index) + 1  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[index_to_word[1]].shape[0]  # define dimensionality of your GloVe word vectors (= 50)\n",
    "\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "\n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False.\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "\n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "\n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_lstm_model(input_shape, word_to_vec_map, word_to_index, index_to_word):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify-v2 model's graph.\n",
    "\n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "\n",
    "    # Create the embedding layer pretrained with GloVe Vectors (â‰ˆ1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index, index_to_word)\n",
    "\n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "\n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    X = LSTM(64, return_sequences=True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.6)(X)\n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    # X = LSTM(128, return_state=True)(X)\n",
    "    X = LSTM(64, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.6)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    # X = Dense(5, activation='softmax')(X)\n",
    "    X = Dense(len(word_to_index))(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation(activation='softmax')(X)\n",
    "\n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txtl_to_X_Y(txtl, word_to_index, nb_of_context_words):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4).\n",
    "\n",
    "    Arguments:\n",
    "    txtl -- list of words (strings)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this.\n",
    "\n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "\n",
    "    # descr of the algo:\n",
    "    1. convert txtl to txtl_indices\n",
    "    1.bis. set m = len(txt) - nb_of_context_words -1\n",
    "    2. initialize X_indices = np.zeros(m, nb_of_context_words)\n",
    "    3. initilize Y_hot = np.zeros(m, vocabulary_size)\n",
    "    4. within loop, set one by one X_indices to corresponding index value\n",
    "    5. same loop, set Y_hot[i, correspesponding_answer_index] = 1\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize X and Y as numpy matrices of zeros and the correct shape\n",
    "    m = len(txtl) - nb_of_context_words - 1  # minus one because each input x_i needs an expected answer y_i\n",
    "    vocab_size = len(word_to_index)\n",
    "    X_indices = np.zeros((m, nb_of_context_words), dtype=np.int32)\n",
    "    Y_hot = np.zeros((m, vocab_size), dtype=np.bool)\n",
    "\n",
    "    txtl_indices = [word_to_index[w] for w in txtl]\n",
    "\n",
    "    for i in range(m):\n",
    "\n",
    "        y_index = txtl_indices[i + nb_of_context_words]\n",
    "        # y_word = txtl[i + nb_of_context_words] # debug purpose only\n",
    "\n",
    "        Y_hot[i, y_index-1] = 1  # y is indexed from 0, words from 1\n",
    "        X_indices[i] = txtl_indices[i: i + nb_of_context_words]\n",
    "        # below alternative to build x same thing with loop and more code\n",
    "        # x_indices = txtl_indices[i: i + nb_of_context_words]\n",
    "        # x_words = txtl[i: i + nb_of_context_words]  # debug purpose only\n",
    "        # for j in range(nb_of_context_words):\n",
    "        #     X_indices[i, j] = x_indices[j]\n",
    "\n",
    "    return X_indices, Y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_inputs(x, y, split_ratio, random=True, seed=None, return_indices=False):\n",
    "    \"\"\" return reduced_x, cross_val_x, reduced_y, cross_val_y\n",
    "    reduced_x has size split_ratio * x.shape[0]\n",
    "    cross_val_x has size (1-split_ratio) * x.shape[0]\n",
    "    involved_indices (optional) is a list of 2 elements: first and second set of involved_indices\"\"\"\n",
    "\n",
    "    if seed: np.random.seed(seed)\n",
    "    assert(x.shape[0] == y.shape[0])\n",
    "\n",
    "    if random:\n",
    "        # shuffle inputs\n",
    "        p = np.random.permutation(x.shape[0])\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = x[p]\n",
    "            y = y[p]\n",
    "        elif isinstance(x, pd.DataFrame) or isinstance(x, pd.Series):\n",
    "            x = x.iloc[p]\n",
    "            y = y.iloc[p]\n",
    "        else:\n",
    "            raise TypeError(\"input type should be ndarray or DataFrame or Series\")\n",
    "    else:\n",
    "        p = np.array(range(x.shape[0]))\n",
    "\n",
    "    # split our training all into 2 sets\n",
    "    n = int(x.shape[0] * split_ratio)\n",
    "    cross_val_x = x[n:]\n",
    "    reduced_x = x[:n]\n",
    "    cross_val_y = y[n:]\n",
    "    reduced_y = y[:n]\n",
    "\n",
    "    if return_indices:\n",
    "        return reduced_x, cross_val_x, reduced_y, cross_val_y, [p[:n], p[n:]]\n",
    "    return reduced_x, cross_val_x, reduced_y, cross_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_prediction(preds, softmax_needed=False, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    # we might prefer not to use a softmax in last layer: in this case we need to 'softmax' output before\n",
    "    # picking one prediction according to model's distrib\n",
    "    if softmax_needed:\n",
    "        preds = np.asarray(preds).astype('float64')  # needed anyway ?\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    out_index = np.random.choice(range(preds.shape[0]), p=probas.ravel())\n",
    "    return out_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text data and glove matrix...\n",
      "nb of words: 257072\n",
      "% of known words in text 0.9919555610879442\n",
      "X_train shape: (47487, 12)\n",
      "X_val shape: (2500, 12)\n",
      "Y_train shape: (47487, 10592)\n",
      "Y_val shape: (2500, 10592)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 12, 52)            550836    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 12, 64)            29952     \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 12, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10592)             688480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 10592)             0         \n",
      "=================================================================\n",
      "Total params: 1,302,292\n",
      "Trainable params: 751,456\n",
      "Non-trainable params: 550,836\n",
      "_________________________________________________________________\n",
      "2500/2500 [==============================] - 4s 1ms/step\n",
      "\n",
      "Test accuracy =  0.0624\n",
      "(1, 12) [[ 4965     3  7731  9297  8808  6172    28  3886     6 10289  4178    28]]\n",
      "It's the story of a ghost, which had a doge after door to expression"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "glove_file = DATA_PATH + 'glove.6B.50d.txt'\n",
    "txt_file = DATA_PATH + 'harry_potter_7.txt'\n",
    "\n",
    "print(\"Loading text data and glove matrix...\")\n",
    "\n",
    "nb_of_context_words = 12\n",
    "\n",
    "textl, word_to_index, index_to_word, word_to_vec_map = prepare_text_and_glove(txt_file, glove_file)\n",
    "\n",
    "nb_words_loaded = 50000\n",
    "textl = textl[:nb_words_loaded]  # memory issues, i have to cut the text :(\n",
    "\n",
    "X_indices, Y_hot = txtl_to_X_Y(textl, word_to_index, nb_of_context_words)\n",
    "x_train, x_test, y_train, y_test = split_inputs(X_indices, Y_hot, split_ratio=0.95)\n",
    "print(\"X_train shape:\", x_train.shape)\n",
    "print(\"X_val shape:\", x_test.shape)\n",
    "print(\"Y_train shape:\", y_train.shape)\n",
    "print(\"Y_val shape:\", y_test.shape)\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "model_file = DATA_PATH + \"simple_lstm_model_\" + str(epochs) + \"_epochs_\" + str(nb_words_loaded) + \"_words.h5\"\n",
    "try:\n",
    "    model = load_model(model_file)\n",
    "    is_loaded = True\n",
    "except:\n",
    "    # Its better to have a decreasing learning rate during the training to reach efficiently the global\n",
    "    # minimum of the loss function.\n",
    "    # To keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically\n",
    "    # every X steps (epochs) depending if it is necessary (when accuracy is not improved).\n",
    "    # With the ReduceLROnPlateau function from Keras.callbacks, i choose to reduce the LR by half if the accuracy\n",
    "    learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, factor=0.6,\n",
    "                                                min_lr=0.0001)\n",
    "\n",
    "    # Define the optimizer\n",
    "    # optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0)\n",
    "\n",
    "    model = create_simple_lstm_model((nb_of_context_words,), word_to_vec_map, word_to_index, index_to_word)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # model.fit(x_train, y_train, epochs=nb_epochs, batch_size=1024, shuffle=True)\n",
    "    history = model.fit(x=x_train, y=y_train, epochs=epochs, batch_size=1024, validation_data=(x_test, y_test),\n",
    "                        verbose=2, callbacks=[learning_rate_reduction])\n",
    "    # Plot the loss and accuracy curves for training and validation\n",
    "    fig, ax = plt.subplots(2, 1)\n",
    "    ax[0].plot(history.history['loss'][5:], color='b', label=\"Training loss\")\n",
    "    ax[0].plot(history.history['val_loss'][5:], color='r', label=\"validation loss\", axes=ax[0])\n",
    "    legend = ax[0].legend(loc='best', shadow=True)\n",
    "\n",
    "    ax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\n",
    "    ax[1].plot(history.history['val_acc'], color='r', label=\"Validation accuracy\")\n",
    "    legend = ax[1].legend(loc='best', shadow=True)\n",
    "    if DISPLAY_GRAPH: plt.show()\n",
    "\n",
    "    is_loaded = False\n",
    "\n",
    "\n",
    "model.summary()\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)\n",
    "\n",
    "#save model\n",
    "if not is_loaded:\n",
    "    save_model(model, model_file)\n",
    "\n",
    "np.random.seed(23)\n",
    "nb_words_to_create = 5\n",
    "first_input_text = \"It's the story of a ghost, which had a\"\n",
    "x = text_to_indices_simple(first_input_text, word_to_index)\n",
    "print(x.shape, x)\n",
    "sys.stdout.write(first_input_text)\n",
    "for i in range(nb_words_to_create):\n",
    "\n",
    "    preds = model.predict(x, verbose=0)[0]\n",
    "    out_index = random_prediction(preds, softmax_needed=True)  # care, indexed from 0\n",
    "    out_word = index_to_word[out_index + 1]  # care, indexed from 1\n",
    "    # print('b', x[0, 1:nb_of_context_words] + [out_index + 1, ])\n",
    "    x[0, 0:nb_of_context_words-1] = x[0, 1:nb_of_context_words]\n",
    "    x[0, nb_of_context_words-1] = out_index + 1  # care, indexed from 1\n",
    "    sys.stdout.write(' ' + out_word)\n",
    "    # print(out_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
